spring:
  datasource:
    dynamic:
      primary: diagnose
      strict: false
      datasource:
        diagnose:
          url: jdbc:mysql://localhost:33066/compass?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai&tinyInt1isBit=false
          username: root
          password: root
        source:
          url: jdbc:mysql://localhost:33066/airflow?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai&tinyInt1isBit=false
          username: root
          password: Root@666

  kafka:
    bootstrap-servers: localhost:9095
    topics: test_airflow_cdc_data
    consumer:
      group-id: test_airflow_cdc_data_22047328_d
      auto-offset-reset: latest
      enable-auto-commit: false
      max-poll-interval-ms: 300000
      security-protocol: "SASL_PLAINTEXT"
      sasl-mechanism: "SCRAM-SHA-512"
      sasl-jaas-config: "org.apache.kafka.common.security.scram.ScramLoginModule required username='hdop' password='v41ieoEFhnPajg5L';"


# 从kafka同步mysql binlog数据
# 必须映射表: user, flow, task, task_instance
datasource:
  mappings:
    # user 表映射
    - schema: "airflow"            # 同步源数据库
      table: "ab_user"      # 源表名
      targetTable: "user"      # 目标表名
      columnMapping: # 字段映射，将id,user_name...字段映射为user_id,username...
        user_id: "id"              # 目标字段:源字段
        username: "username"
        password: "password"
        email: "email"
        create_time: "created_on"
        update_time: "changed_on"
      constantColumn:
        scheduler_type: "Airflow"

    # project 表映射
    - schema: "airflow"
      table: "dag"
      targetTable: "project"
      columnMapping:
        id: ""
        project_name: "dag_id"
        description: "description"
        user_id: ""
        project_status: "is_active"
        create_time: ""
        update_time: "last_updated"
      columnDep:
        columns: [ "user_id" ]
        queries: [ "select id as user_id from ab_user where username ='admin' limit 1" ]


    # flow 表映射
    - schema: "airflow"
      table: "dag"
      targetTable: "flow"
      columnMapping:
        id: ""
        flow_name: "dag_id"
        description: "description"
        user_id: ""
        flow_status: "is_active"
        project_id: ""
        project_name: "dag_id"
        update_time: "last_updated"
      columnDep:
        columns: [ "user_id" ]
        queries: [ "select id as user_id from ab_user where username ='admin' limit 1" ]

    # task 任务表映射
    - schema: "airflow"
      table: "task_instance"
      targetTable: "task"
      columnMapping:
        id: ""
        project_name: "dag_id"
        project_id: ""
        flow_name: "dag_id"
        flow_id: ""
        task_name: "task_id"
        user_id: ""
        task_type: "operator"
        retries: "max_tries"
        create_time: ""
        update_time: ""
      columnDep:
        columns: [ "user_id", "update_time" ]
        queries: [ "select id as user_id from ab_user where username ='admin' limit 1" ]

    # task_instance 任务表映射
    - schema: "airflow"
      table: "task_instance"
      targetTable: "task_instance"
      columnMapping:
        id: ""
        project_name: "dag_id"
        flow_name: "dag_id"
        task_name: "task_id"
        start_time: "start_date"
        end_time: "end_date"
        execution_time: "execution_date"
        task_state: "state"  # 映射
        task_type: "operator"
        retry_times: "try_number"
        max_retry_times: "max_tries"
        worker_group: ""
        create_time: ""
        update_time: ""
        run_id: "run_id"
      columnValueMapping: # 字段值映射
        # 只需要关注成功和失败，其他状态为其他
        task_state:
          - { targetValue: "success", originValue: [ "success" ] }
          - { targetValue: "fail", originValue: [ "failed" ] }
          - { targetValue: "up_for_retry", originValue: [ "up_for_retry" ] }
          - { targetValue: "other", originValue: [ "none", "removed", "scheduled", "queued", "running", "shutdown", "up_for_reschedule", "upstream_failed", "skipped","sensing","deferred" ] }
      # 写回 kafka topic
      writeKafkaTopic: compass_test
